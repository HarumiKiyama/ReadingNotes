#+TITLE: Introduction to Algorithms 3th
#+STARTUP: overview
* 提出问题
- 全书讲了什么
- 全书是如何组织的
- 这本书和我有什么关系
* 阅读的目的
学习基本的算法为之后找到更好的工作打下基础,大概在3个月之内过一遍这本书,习题基本上每章的题都做一遍吧.
* 书籍的简单摘要
This book provides a comprehensive introduction to the modern study of computer algorithms
* 结构
** Fundations
*** Introduction
The fundation of algorithms
*** The Role of Algorithms in Computing
overview of algorithms and their place in modern computing systems
An =Algorithms= is any well-defined computational procedure that takes some value, or set of values, as input and produces some value, or set of values, as output. In other words, An algorithms is thus a sequence of computational steps that transform the input into output.
An =Algorithms= is correct if, for every input instance, it halts with the correct output.
*** Getting Started
**** insertion sort
#+BEGIN_SRC python :results output
  def sort(a: [int]) -> [int]:
      for j in range(1,len(a)):
          key = a[j]
          i = j - 1
          while i >= 0 and a[i] > key:
              a[i + 1] = a[i]
              i = i - 1
          a[i + 1] = key
      return a
  print(sort([1, 0, 4, 5, 2231, 312, 3244]))
#+END_SRC
use loop invariant to show why an algorithms is correct, the same as mathematical induction
- Initialization
  It is true prior to the first iteration of the loop
- Maintenance
  If it is true before an iteration of the loop, it remains true before the next iteration
- Termination
  When the loop terminates, the invariant gives us a useful property that helps show that the algorithm is correct
**** merge sort
devide-and-conquer approach
- =Divide= the problem into a number of subproblems that are smaller instances of the same problem
- =Conquer= the subproblems by solving them recursively
- =Combine= the solutions to the subproblems into the solution for the original problem
**** Analyze merge sort
- =Divide= the divide step just computes the middle of the subarray, which takes constant time. Thus, $$D(n)=\Theta(1)$$
- =Conquer= We recursively solve two subproblems, each of size $$n/2$$, which contributes $$2T(n/2)$$ to running time
- =Combine= We have already noted that the Merge procedure on an n-element subarray takes time $$\Theta(n)$$, and so $$C(n)=\Theta(n)$$
*** Growth of Functions
precisely defines asymptotic notation
**** Theta-notation
$$\Theta(g(n)) = \{f(n) : \text{ there exist positive constants }c_1,c_2,\text{ and } n_0 \text{ such that } 0 \leq c_1 g(n) \leq f(n) \leq c_2 g(n) \text{ for all } n \geq n_0 \}$$
**** O-notation
$$O(g(n)) = \{f(n) : \text{ there exist positive constants } c \text{ and } n_0 \text{ such that } 0 \leq f(n) \leq cg(n) \text{ for all } n \geq n_0 \}$$
$$\Theta(g(n)) \subseteq O(g(n))$$
**** Omega-notation
$$\Omega(g(n)) = \{f(n) : \text{ there exist positive constants } c \text{ and } n_0 \text{ such that } 0 \leq cg(n) \leq f(n) \text{ for all } n \geq n_0 \}$$
$$\Theta(g(n)) \subseteq \Omega(g(n))$$
**** o-notation
denote an upper bound that is not asymptotically tight
$$o(g(n)) = \{f(n) : \text{ for any positive constant } c > 0 \text{ there exists a constant } n_0 > 0 \text{ such that } 0 \leq f(n) \leq cg(n) \text{ for all } n \geq n_0 \}$$
$$\lim_{n \to \infty} \frac{f(n)}{g(n)} = 0$$
**** omega-notation
denote an lower bound that is not asymptotically tight
$$\omega(g(n)) = \{f(n) : \text{ for any positive constant } c > 0 \text{ there exists a constant } n_0 > 0 \text{ such that } 0 \leq cg(n) \leq f(n) \text{ for all } n \geq n_0 \}$$
$$\lim_{n \to \infty} \frac{f(n)}{g(n)} = \infty$$
*** Divide-and-Conquer
Delve into divide-and-conquer method.
**** Recurrences
A recurrences is an equation or inequality that describes a function in terms of its value on smaller inputs.
for example:
$$T(n) = \begin{cases} \Theta(1) &\quad \text{if } n = 1\\ 2T(n/2)+\Theta(n) &\quad \text{if } n > 1 \end{case}$$
Three methods to solve recurrence and obtain asymptotic "\Theta", "O"
- substitution method
  guess a bound and then use mathematical induction to prove it
- recursion-tree method
  convert the recurrence into a tree whose nodes represent the coasts incurred at various levels of the recursion. and use techniques for bounding summations to solve the recurrence.
- master method
  provide bounds for recurrences of the form $$T(n)=aT(n/b)+f(n)$$
**** The maximum-subarray problem
using Dynamic programming time coast $$\Theta(n)$$
**** matrix multiplication
**** The substitution method for solving recurrences
1. guess the form of the solution
2. use mathematical induction to find the constants and show that solution works
**** The master method for solving recurrences
Let $$a\geq 1$$ and $$b > 1$$ be constants, let $$f(n)$$ be a function, and let $$T(n)$$ be defined on the nonnegative integers by the recurrence
$$T(n)=aT(n/b)+f(n)$$
then $$T(n)$$ has the follow-ing asymptotic bounds:
1. If $$f(n)=O(n^{\log_{b} a-\epsilon})$$ for some constant $$\epsilon > 0$$, then  $$T(n)=\Theta(n^{\log_b a})$$
2. If $$f(n)=\Theta(n^{\log_b a})$$ , then $$T(n)=\Theta(n^{\log_b a}\lg n)$$
3. If $$f(n)=\Omega(n^{\log_b a+\epsilon})$$ for some constant $$\epsilong > 0$$, and if $$af(n/b) \leq cf(n)$$ for some constant $$c<1$$ and all sufficiently large $$n$$, then $$T(n)=\Theta(f(n))$$
*** Probabilistic Analysis and Randomized Algorithms
Introduce probailistic analysis and randomized algorithms
** Sorting and Order Statistics
*** Introduction
| Algorithm      | Worst-case         | Average-case/expected         |
|                | running time       | running time                  |
|----------------+--------------------+-------------------------------|
| Insertion sort | $$\Theta(n^2)$$    | $$\Theta(n^2)$$               |
| Merge sort     | $$\Theta(n\lg n)$$ | $$\Theta(n\lg n)$$            |
| Heapsort       | $$\Theta(n\lg n)$$ | --                            |
| Quicksort      | $$\Theta(n^2)$$    | $$\Theta(n\lg n)$$ (expected) |
| Counting sort  | $$\Theta(k+n)$$    | $$\Theta(k+n)$$               |
| Radix sort     | $$\Theta(d(n+k))$$ | $$\Theta(d(n+k))$$            |
| Bucket sort    | $$\Theta(n^2)$$    | $$\Theta(n)$$ (average-case)  |
*** Heapsort
#+BEGIN_SRC python :results output
  class NewList:
      def __init__(self, l):
          self._l = l
          self.heap_size = len(l)

      def __len__(self):
          return len(self._l)

      def __getitem__(self, n):
          return self._l[n - 1]

      def __setitem__(self, n, val):
          self._l[n - 1] = val

      @classmethod
      def from_list(cls, l):
          return cls(l)

      def __repr__(self):
          return self._l.__repr__()


  def parent(i):
      return i / 2


  def left(i):
      return 2 * i


  def right(i):
      return 2 * i + 1


  def max_heapify(A, i):
      l = left(i)
      r = right(i)
      if l <= A.heap_size and A[l] > A[i]:
          largest = l
      else:
          largest = i
      if r <= A.heap_size and A[r] > A[largest]:
          largest = r
      if largest != i:
          exchange(A, i, largest)
          max_heapify(A, largest)


  def exchange(l, i, j):
      l[i], l[j] = l[j], l[i]


  def build_max_heap(A):
      A.heap_size = len(A)
      for i in range(len(A) // 2, 0, -1):
          max_heapify(A, i)


  def heapsort(A):
      build_max_heap(A)
      for i in range(len(A), 1, -1):
          exchange(A, 1, i)
          A.heap_size -= 1
          max_heapify(A, 1)


  def main():
      l = NewList.from_list([4, 1, 5, 0, 7, 9, 8])
      heapsort(l)
      print(l)


  if __name__ == '__main__':
      main()
#+END_SRC

#+RESULTS:
: [0, 1, 4, 5, 7, 8, 9]
*** Quicksort
quicksort is often the best practical choice for sorting
#+BEGIN_SRC python :results output
  import random


  def QUICKSORT(A, p, r):
      if p < r:
          q = PARTITION(A, p, r)
          QUICKSORT(A, p, q - 1)
          QUICKSORT(A, q + 1, r)


  def PARTITION(A, p, r):
      x = A[r]
      i = p - 1
      for j in range(p, r - 1):
          if A[j] <= x:
              i += 1
              A[i], A[j] = A[j], A[i]
      A[i + 1], A[r] = A[r], A[i + 1]
      return i + 1


  def RANDOMIZED_PARTITION(A, p, r):
      i = random.randint(p, r)
      A[r], A[i] = A[i], A[r]
      return PARTITION(A, p, r)


  def RANDOMIZED_QUICKSORT(A, p, r):
      if p < r:
          q = RANDOMIZED_PARTITION(A, p, r)
          RANDOMIZED_QUICKSORT(A, p, q - 1)
          RANDOMIZED_QUICKSORT(A, q + 1, r)
#+END_SRC
In worst-case quicksort coasts $$\Theta(n^2)$$ , however in best-case coasts $$\Theta(n\lg n)$$. If partitions alternate between worst-case and best-case, the coast is also $$\Theta(n\lg n)$$.
randomized version of quicksort is the sorting algorithm of choice for large enough inputs.
*** Sorting in Linear Time
Any comparison sort must make $$\Omega(n\lg n)$$ comparisons in the worst case to sort n elements
**** counting sort
#+BEGIN_SRC python
  def COUNT_SORT(A, B, k):
      c = [0 for _ in range(k + 1)]
      for j in range(len(A)):
          c[A[j]] = c[A[j]] + 1
      for i in range(k + 1):
          c[i] = c[i] + c[i - 1]
      for j in range(len(A) - 1, 0, -1):
          B[c[A[j]]] = A[j]
          c[A[j]] = c[A[j]] - 1
#+END_SRC
**** radix sort
**** bucket sort
*** Medians and Order Statistics
** Data Structures
** Advanced Design and Analysis Techniques
** Advanced Data Structures
** Graph Algorithms
** Selected Topics
** Appendix
* 习题集
* 吐槽
